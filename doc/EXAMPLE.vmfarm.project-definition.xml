<?xml version="1.0" encoding="UTF-8"?>
<!-- 
     This example definition files contains a project with multiple
     networks configured within a single VLAN. This is for multi-homed
     hosts and storage, such as those used for VMWare ESX server farms.
  -->

<!-- This use of DOCTYPE with a declared ENTITY lets us include
     XML from another file inside this one. This is useful when
     the XML we want to include isn't well formed, which is the
     case for VMfarm <vlan/> and <host/> definitions.
     That's why we can't use <xi:include/>
-->
<!DOCTYPE project SYSTEM "http://docgen.eigenmagic.com/docgen.dtd" [

<!ENTITY vmfarm-includes SYSTEM "EXAMPLE.vmfarm-includes.xml">

]>

<project name="myproj" code="02" description="VMware demo project">

  <background>
    <para>The environment consists of Solaris servers.
    </para>

    <itemizedlist>
      <listitem>
	<para>3 x Sun Solaris Servers
	</para>
      </listitem>

    </itemizedlist>

  </background>

  <!-- Design revision history -->
  <revision 
     majornumber="1"
     minornumber="0"
     date="22 July 2009"
     author="JPW"
     reviewer="Someone"
     reviewdate="23 July 2009"/>

  <database id="MYDB01" type="Oracle">
    <onhost name="primhost01"/>
    <onhost name="primhost02"/>
  </database>

  <site name="sitea" type="primary" location="PrimarySite">

    <vlan type="project" number="3003" mtu='2447'>
      <network number="10.240.4.0/24" gateway="10.240.4.254"/>
      <network number="10.241.9.0" netmask="255.255.255.0" gateway="10.241.9.254"/>
    </vlan>

    <vlan type="services" number="1544">
      <network number="10.10.10.0" netmask="255.255.255.0" gateway="10.459.11.336"/>
    </vlan>
    <vlan type="services" number="1589">
      <network number="10.80.90.0" netmask="255.255.255.0" gateway="10.189.14.8"/>
    </vlan>

    <!-- load in the VMfarm definitions -->
    &vmfarm-includes;

    <host name="primhost01"
          operatingsystem="solaris10"
          location="Primary"
          description="Database server">

      <!-- The IP address assigned to the host on the IP-SAN. This is usually
	   assigned by the network team from a network range specifically for
	   this project, tracked in the solution control spreadsheets.
	-->

      <netinterface type='storage' mode='active'>
	<switchname>sitea-swtedg-0103</switchname>
	<switchport>GigabitEthernet1/1</switchport>
	<hostport>PCI 0</hostport>
	<ipaddr>10.240.4.232</ipaddr>
      </netinterface>

      <netinterface type='storage' mode='passive'>
	<switchname>sitea-swtedg-0104</switchname>
	<switchport>GigabitEthernet1/1</switchport>
	<hostport>PCI 1</hostport>
      </netinterface>

      <netinterface type='storage' mode='active'>
	<ipaddr>10.240.4.112</ipaddr>
      </netinterface>

      <iscsi_initiator>an.iscsi.initname</iscsi_initiator>
      <drhost name='drtesthost01'/>

    </host>

    <!-- Continue to add hosts, one at a time, for every host in the project. -->
    <host name="primhost02"
          operatingsystem="solaris10"
          location="Primary"
          description="Database server">

      <!-- demo of a trunked netinterface -->
      <netinterface type='storage' mode='trunk' mtu='1500'>
	<vlan_number>3031</vlan_number>
	<vlan_number>3032</vlan_number>
	<switchname>sitea-swtedg-0103</switchname>
	<switchport>GigabitEthernet1/2</switchport>
	<hostport>PCI 0</hostport>
      </netinterface>

      <drhost name="drtesthost01"/>
      <drhost name="drtesthost02"/>

    </host>

    <host name="primhost03"
          operatingsystem="solaris10"
          location="Primary"
          description="Database server">

      <netinterface type='storage' mode='active'>
	<switchname>sitea-swtedg-0103</switchname>
	<switchport>GigabitEthernet1/3</switchport>
	<hostport>PCI 0</hostport>
	<ipaddr>10.240.4.235</ipaddr>
      </netinterface>

      <drhost name="drtesthost02"/>

    </host>

    <host name="virtualhost-01" virtual="yes"
          operatingsystem="solaris10"
          location="Primary"
          description="Virtual host on primhost02">

      <netinterface type='storage' mode='active'>
	<!--      <switchname>sitea-swtedg-0103</switchname>
		  <switchport>GigabitEthernet1/3</switchport>
		  <hostport>PCI 0</hostport> -->
	<ipaddr>10.240.4.238</ipaddr>
      </netinterface>

      <drhost name="drtesthost02"/>

    </host>

    <host name="examplehost-01" virtual="yes"
          operatingsystem="solaris10"
          location="Primary"
          description="An example virtual host for sorting">

      <netinterface type='storage' mode='active'>
	<!--      <switchname>sitea-swtedg-0103</switchname>
		  <switchport>GigabitEthernet1/3</switchport>
		  <hostport>PCI 0</hostport> -->
	<ipaddr>10.240.4.201</ipaddr>
      </netinterface>

    </host>

    <filer type="filer" name="sitea-fashda-01" partner="sitea-fashdb-01">

      <!-- A vfiler must be defined for the primary.
	   - If no 'name' attribute is specified, it defaults to the same
	   as the project shortname. This will be the case for 99% of
	   designs, and the generator may break if you actually specify a name.
	   - The 'rootaggr' attribute must be supplied. It defines which
	   aggregate on the filer will be used for the vfiler root volume.
	-->
      <vfiler>
        <aggregate type="root" name="aggr01"/>

	<protocol name="nfs"/>
	<protocol name="cifs"/>

	<!--
	    Each filer must have a primaryip. This is the IP address used to
	    provide the storage to the hosts.
	    You will also need to supply the netmask.
	  -->
        <ipaddress type="primary" ip="10.240.4.225"/>
        <ipaddress type="alias" ip="10.240.4.224"/>

	<!-- Define vlanip tags if you need a services VLAN.
	     This IP address will be added to the vfiler on an netinterface
	     in the services VLAN so you can access external services, such
	     as ActiveDirectory for doing CIFS authentication.
	  -->
	<ipaddress type="service" vlan_number="1544" ip="10.10.10.15"/>
	<ipaddress type="service" vlan_number="1589" ip="10.80.90.12"/>

	<!-- volumes exist within aggregates, so for each aggregate you
	     want to use, define an aggregate like this.
	  -->

	<aggregate name="aggr02">
          <!-- An Oracle quorum volume. Every project with databases
                 will need one of these. It isn't backed up or mirrored.
              -->
	  <volume type="oracm" snapreserve="0" usable="1"/>

	  <!--
	      The 'oraconfig' volume is defined second. There is only
	      one of these per Oracle RAC project. It should be backed up.
	    -->
	  <volume type="oraconfig" usable="5">
	    <setref type="snapvault" name="default_primary"/>
	  </volume>

	  <!-- MYDB01 database -->
	  <volume oracle="MYDB01" type="oradata" usable="60">
	    <!-- The 'oradata' volume type is used for the Oracle data area -->

	    <!-- The autosize parameter allows the volume to automatically grow
		 to a maximum size, growing by 'increment' each time.
		 the size is a number, followed by a size indicator (300g, 2t, etc)
		 and the increment is also a number with a size
	    -->
	    <autosize max="500g" increment="10g"/>

	    <!-- autodelete defines the way in which volume snapshots will be
		 automatically deleted if they start using too much space.
		 This follows the syntax of the 'snap autodelete' command.
		 It defaults to the following options:
		    commitment = 'try'
		    trigger = 'volume'
		    target_free_space = '80' (%)
		    delete_order = oldest_first
		    defer_delete = none
		    prefix = '' This isn't used unless defer_delete is set to 'prefix'
	    -->
	    <autodelete />

	    <setref type="snapvault" name="MYDB01-data"/>
	    <setref type="snapmirror" name="default_sm"/>
	  </volume>

	  <volume oracle="MYDB01" type="oraindx" usable="30">
	    <!-- The 'oraindx' type is for an oracle index volume. -->
	    <setref type="snapvault" name="MYDB01-data"/>
	    <setref type="snapvaultmirror" name="default_svm"/>
	    <setref type="snapmirror" name="default_sm"/>
	  </volume>

	  <volume oracle="MYDB01" type="oraredo" usable="8"/>

	  <volume oracle="MYDB01" type="oraundo" usable="20"/>

	  <volume oracle="MYDB01" type="oraarch" usable="23">
	    <setref type="snapmirror" name="default_sm"/>
	    <setref type="snapvault" name="MYDB01-arch"/>
	  </volume>

	  <!-- This is a standard volume, which will default to being
	       exported to all hosts via NFS.
	    -->
	  <volume usable="58" snapstorage="150">
	    <setref type="snapvault" name="custom-multiplier"/>	      
	  </volume>

	  <!-- We might want to restart the automatic volume numbering
	       for some reason, such as virtualised environments where
	       we want the 'test' suffixed volume to have the same
	       number as the production volume.
	       This can also be used to number a volume arbitrarily, but
	       have all the other automatic naming details used as normal.
	    -->
	  <volume restartnumbering="19" usable="50">
	    <setref type="snapvault" name="custom-storage"/>
	  </volume>

	  <!-- This is a volume with some manual qtree definitions -->
	  <volume oplocks='no' usable="120">
	    <qtree name="my_first_qtree_01" description="Demo Manual Qtree 1"/>
	    <qtree name="my_first_qtree_02" description="Demo Manual Qtree 2"/>
	  </volume>

	  <!-- This is an iSCSI volume that contains a couple of LUNs -->

	  <volume proto="iscsi" usable="100">
	    <lun>
	      <export to='primhost01'/>
	    </lun>

	    <setref type="snapvault" name="MYDB01-arch"/>
	    <setref type="snapmirror" name="default_sm"/>
	  </volume>

	  <!-- A volume with a specific qtree name, and a LUN automatically placed inside it. -->
	  <volume proto="iscsi" name="iscsi_test" usable="100">
	    <qtree name="my_custom_name" description="A custom qtree with a LUN in it">
	      <lun lunid="103"/>
	    </qtree>

	    <export to="primhost01"/>
	    <setref type="snapvault" name="MYDB01-arch"/>
	  </volume>

	  <!-- A volume with multiple qtrees, a LUN defined outside the qtree, and
	       a LUN inside one of the qtrees. Only the LUN inside the qtree, and
	       that qtree, will be included in the configuration, as the 'external'
	       style of defining the LUN will be overridden by the more specific method.
	       
	       The attribute 'iscsi_snapspace' is used to define an area of usable
	       space that will be used for storing snapshots. This parameter defaults
	       to 30 for iscsi volumes that have snapshots, and is ignored for non-iscsi
	       volumes, or iscsi volumes that do not have snapshots.
	       The value is a percentage (eg: 25 == 25%) of the usablestorage value for
	       the volume.
	    -->
	  <volume proto="iscsi" iscsi_snapspace="50" usable="100">
	    <qtree name="my_custom_name" description="A custom qtree with a LUN in it"/>

	    <qtree name="lun_inside" description="A custom qtree with 2 LUNs in it">
	      <lun restartnumbering="80"/>
	      <lun/>
	    </qtree>

	    <lun/> <!-- this lun will be ignored -->

	    <export to="primhost01"/>

	    <setref type="snapvault" name="MYDB01-arch"/>
	  </volume>

	  <volume proto="cifs" usable="50"/>

	</aggregate>

	<!-- Here we can define some volumes on a different aggregate from the
	     first one.
	  -->
	<aggregate name="aggr03">
	  <autosize max="500g" increment="10g"/>

	  <volume name="wibble">
	    <!-- Some custom export options -->
	    <!-- An export from an alias IP address on a vfiler -->
	    <export to="primhost01" fromip="10.44.3.78">
	      <mountoption>forcedirectio</mountoption>
	      <mountoption>clownage=true</mountoption>
	    </export>

	    <!-- An export to a second primary storage interface on a host -->
	    <export to="primhost03" ro="yes" toip='10.240.4.112'/>

	    <!-- Add an alias for the export -->
	    <exportalias>/a/fake/export/alias</exportalias>
	    <exportalias>/another/fake/export/alias</exportalias>

	    <setref type="snapmirror" name="default_sm"/>
	  </volume>

	  <volume name="wibble2"/>

	</aggregate>

      </vfiler>

    </filer>

    <!-- The nearstore filer is used for the backups from the primary filer.
	 Very little needs to be defined for the nearstore, as most of the
	 definitions are contained in the primary volumes and their references
	 to snapvaultsets.
      -->
    <filer type="nearstore" name="sitea-fasnst-01">

      <!-- A vFiler with a rootaggr is required, so we know where to create
	   the root volume. The name of the vFiler will match that of the primary filer.
	-->
      <vfiler>
        <aggregate type="root" name="aggr09"/>

	<!-- The NearStore has its own IP address in the project VLAN -->
        <ipaddress type="primary" ip="10.240.4.226"/>
        <ipaddress type="alias" ip="10.240.4.223"/>

	<ipaddress type="service" vlan_number="3544" ip="10.10.10.16"/>

	<aggregate name="aggr02">
	  <autosize max="500g" increment="25g"/>
	  <autodelete />
	</aggregate>

      </vfiler>
    </filer>

  </site>

  <!-- DR configuration

       If you have a DR setup for the project, define it here.
       The site type is 'secondary' for the DR site.
    -->

  <site name='siteb' type="secondary" location="DisasterRecoverySite">

    <!-- The DR site also has a vlan, and it may have a different vlan number
	 to the primary site. Its IP network is likely to be different, also,
	 so you will need to have a different gateway IP address.
      -->
    <vlan type="project" number="3113">
      <network number="10.140.8.15/27" gateway="10.241.XXX.XXX"/>
    </vlan>

    <!-- Some hosts that are purportedly at DR -->
    <host name="drtesthost01"
          operatingsystem="solaris10"
          location="Secondary"
          description="Database server">

      <netinterface type='storage' mode='active'>
	<switchname>siteb-swtedg-0101</switchname>
	<switchport>GigabitEthernet1/6</switchport>
	<hostport>PCI 0</hostport>
	<ipaddr>10.XXX.123.1</ipaddr>
      </netinterface>

      <!-- a demo of a host with multiple active storage IPs
	   such as something using iSCSI MPIO on windows.
	-->

      <netinterface type='storage' mode='active'>
	<switchname>siteb-swtedg-0101</switchname>
	<switchport>GigabitEthernet1/8</switchport>
	<hostport>PCI 0</hostport>
	<ipaddr>10.XXX.123.16</ipaddr>
      </netinterface>


      <iscsi_initiator>another.iscsi.initname</iscsi_initiator>

    </host>

    <host name="drtesthost02"
          operatingsystem="solaris10"
          location="Secondary"
          description="Database server">

      <netinterface type='storage' mode='active'>
	<switchname>siteb-swtedg-0101</switchname>
	<switchport>GigabitEthernet1/2</switchport>
	<hostport>PCI 0</hostport>
	<ipaddr>10.XXX.123.2</ipaddr>
      </netinterface>

    </host>

    <filer type="filer" name="siteb-fashda-01" partner="siteb-fashdb-01">
      <vfiler>
        <aggregate type="root" name="aggr02"/>
        <ipaddress type="primary" ip="10.241.XXX.1"/>
	<aggregate name="aggr17">

	  <volume name="custom_snapvault_test" usable="146">
	    <setref type="snapvault" name="specific"/>
	  </volume>

	</aggregate>

      </vfiler>

    </filer>

    <filer type="nearstore" name="siteb-fasnst-01">
      <vfiler>
        <aggregate type="root" name="aggr09"/>
        <ipaddress type="primary" ip="10.240.XXX.XXX"/>

	<aggregate name="aggr02">
	  <autodelete />

	  <volume name="mycustomvolume" space_guarantee="volume"/>

	</aggregate>

      </vfiler>
    </filer>

  </site>

  <!--
      The SnapVault and SnapMirror definitions are put here. These define
      how the backups and DR copies are actually done. The volumes you defined
      above refer to these 'snapvaultset' and 'snapmirrorset' nodes by
      referring to their 'id' attribute.
    -->

  <!-- default snapvaults -->
  <!-- You will need to define 1 snapvaultset as a minimum, with the id 'default_primary'.
       This is the snapvaultset used to back up the root volume on the primary filer.
       You can use the same snapvaultset to back up data volumes as well, if you wish.
    -->

  <!-- 
       A snapvaultset has 3 attributes:
       - id: a unique identifier for the snapvaultset, used when referring to it from a
       volume definition, via the snapvaultsetref parameter.
       - targetfiler: This is the target filer the snapvault should copy data to. This
       should always be the NearStore at the same site as the volume
       being snapvaulted.
       - targetaggr: This is the aggregate on the target NearStore that the snapvaulted
       volumes should be created in. You may wish to use a different
       aggregate on the NearStore than the one you use on the primary filer,
       usually because the NearStore has a lot more aggregates than the
       primary. You can also make different volumes on the primary get
       snapvaulted to different aggregates. This is particularly useful
       for large volumes, such as large databases, where you might want to
       snapvault the data volume to aggr03, but put the indexes on aggr04, 
       for example.
       A snapvaultset has one type of child node: <snapvault/>. You can have multiple
       <snapvault/> nodes, defining different snapvault schedules. You will generally
       have 2: sv_daily and sv_weekly.
       The <snapvault/> node has 2 children: <snapschedule/> and <snapvaultschedule/>.
       They are both snapvault schedule definitions. The <snapschedule/> defines the
       schedule for the baseline snapshot on the primary filer, and the <snapvaultschedule/>
       defines the transfer schedule on the NearStore, defining when the data is actually
       snapvaulted from the primary to the NearStore.
       The schedule is of the form: number@days_of_the_week@hour
       Make sure the snapvaultschedule happens after the snapschedule, so that the data
       changes are snapshotted on the primary before the transfer is attempted.
       
       If you configure a snapvaultset with a snapvaultschedule, but no snapschedule,
       instead of performing a snapvault transfer, the design will be configured to take
       a local snapvault snapshot on the sceondary device. This assumes that you do have
       a snapvault that contains both a snapschedule and a snapvaultschedule, so that
       some form of transfer takes place, otherwise the local snapvault snapshot won't
       have any changed data to snapshot.
       
       eg: To back up a primary volume every day to a secondary, and to take a weekly
       snapshot locally on the secondary, you would configure a snapvault set like this:

       <snapvaultset id="default_primary" targetfiler="sitea-fasnst-01" targetaggregate="aggr02">
	 <snapvault basename="sv_daily">
	   <snapschedule>1@3</snapschedule>
	   <snapvaultschedule>14@4</snapvaultschedule>
	 </snapvault>
	 <snapvault basename="sv_weekly">
	   <snapvaultschedule>4@sun@5</snapvaultschedule>
	 </snapvault>
       </snapvaultset>

       -->

  <!-- primary site -->
  <snapvaultset id="default_primary" targetfiler="sitea-fasnst-01" targetaggregate="aggr02">
    <snapvault basename="sv_daily">
      <snapschedule>1@1</snapschedule>
      <snapvaultschedule>8@2</snapvaultschedule>
    </snapvault>
    <snapvault basename="sv_weekly">
      <snapvaultschedule>13@sun@3</snapvaultschedule>
    </snapvault>
  </snapvaultset>

  <!-- DR site -->
  <!-- The 'default_secondary' snapvaultset is used by the secondary site for the
       backup snapvaults of the priamry filer at the secondary site. Note how the
       targetfiler is the NearStore at the same site as the DR primary.
    -->
  <snapvaultset id="default_secondary" targetfiler="siteb-fasnst-01" targetaggregate="aggr02">
    <snapvault basename="sv_daily">
      <snapschedule>1@1</snapschedule>
      <snapvaultschedule>8@2</snapvaultschedule>
    </snapvault>
    <snapvault basename="sv_weekly">
      <snapvaultschedule>13@sun@3</snapvaultschedule>
    </snapvault>
  </snapvaultset>


  <!-- backup snapshots for MYDB01 -->
  <!-- Here we have some example database snapshots. In order to correctly back up
       an Oracle database, the data must be snapshotted before the archivelogs
       are snapshotted. This has to do with the way hotbackup mode works with
       archivelogs.
       In general, the <snapschedule/> for the data snapvaults should be an hour
       before the one used for the archivelogs.
    -->
  <snapvaultset id="MYDB01-data" targetfiler="sitea-fasnst-01" targetaggregate="aggr02">
    <snapvault basename="sv_daily">
      <snapschedule>1@1</snapschedule>
      <snapvaultschedule>8@2</snapvaultschedule>
    </snapvault>
    <snapvault basename="sv_weekly">
      <snapvaultschedule>13@sun@3</snapvaultschedule>
    </snapvault>
  </snapvaultset>

  <!-- This is the corresponding archivelog snapvaultset for the data snapvaultset above -->
  <snapvaultset id="MYDB01-arch" targetfiler="sitea-fasnst-01" targetaggregate="aggr02">
    <snapvault basename="sv_daily">
      <snapschedule>1@1</snapschedule>
      <snapvaultschedule>8@2</snapvaultschedule>
    </snapvault>
    <snapvault basename="sv_weekly">
      <snapvaultschedule>13@sun@3</snapvaultschedule>
    </snapvault>
  </snapvaultset>

  <!-- This is a custom snapvault relationship that uses specific multiplier
       to define how much storage should be created on the target volume
       instead of the default (2.5, usually).
    -->
  <snapvaultset id="custom-multiplier" multiplier="6"
		targetfiler="sitea-fasnst-01" targetaggregate="aggr09">
    <snapvault basename="sv_daily">
      <snapschedule>1@1</snapschedule>
      <snapvaultschedule>8@2</snapvaultschedule>
    </snapvault>
    <snapvault basename="sv_weekly">
      <snapvaultschedule>13@sun@3</snapvaultschedule>
    </snapvault>
  </snapvaultset>

  <snapvaultset id="custom-storage" targetusable="2244"
		targetfiler="sitea-fasnst-01" targetaggregate="aggr09">
    <snapvault basename="sv_daily">
      <snapschedule>1@1</snapschedule>
      <snapvaultschedule>8@2</snapvaultschedule>
    </snapvault>
    <snapvault basename="sv_weekly">
      <snapvaultschedule>13@sun@3</snapvaultschedule>
    </snapvault>
  </snapvaultset>

  <snapvaultset id="specific" targetfiler="siteb-fasnst-01" targetvolume="mycustomvolume">
    <snapvault basename="custom_daily">
      <snapschedule>1@1</snapschedule>
      <snapvaultschedule>8@2</snapvaultschedule>
    </snapvault>
    <snapvault basename="custom_weekly">
      <snapvaultschedule>3@sun@3</snapvaultschedule>
    </snapvault>
  </snapvaultset>
  <!--
      SnapMirrors
      
      SnapMirrors are a bit different to SnapVaults, though the underlying technology is
      very similar. They are defined on the filer by establishing a relationship in a
      very similar way to SnapVaults, which is why the attributes for the <snapmirrorset/>
      are the same as for SnapVaults. SnapMirrors use a configuration file on the filer,
      /etc/snapmirror.conf, to provide the schedules for the snapmirrors on the filer.
      
      The <snapmirror/> child node is used to define these schedules. There should only be one
      <snapmirror/> child node for each <snapmirrorset/>, except in unique circumstances.
      The <snapmirror/> node has 4 parameters:
      - minute: the minute(s) of the hour to perform the snapmirror
      - hour: the hour(s) to perform the snapmirror
      - dayofmonth: The day(s) of the month to perform the snapmirror
      - dayofweek: The day(s) of the week to perform the snapmirror.
      
      If any of the parameters are omitted, it will default to '*', ie: all/any

      -->

  <!-- snapmirrors -->
  <snapmirrorset id="default_sm" targetfiler="siteb-fashda-01" targetaggregate="aggr02">
    <snapmirror>
      <minute>0</minute>
      <hour>6</hour>
      <dayofmonth>*</dayofmonth>
      <dayofweek>*</dayofweek>
    </snapmirror>
  </snapmirrorset>

  <!-- snapvault snapmirrors -->
  <!-- SnapVault snapmirrors are defined the same as regular snapmirrors.
       It is the way they are referenced by a volume that alters the way they
       are set up.
    -->
  <snapmirrorset id="default_svm" targetfiler="siteb-fasnst-01" targetaggregate="aggr02">
    <snapmirror>
      <minute>0</minute>
      <hour>6</hour>
      <dayofmonth>*</dayofmonth>
      <dayofweek>*</dayofweek>
    </snapmirror>
  </snapmirrorset>

</project>
